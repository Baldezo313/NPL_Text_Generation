{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baldezo313/NPL_Text_Generation/blob/main/src/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Génération de texte avec les réseaux de neurones  \n",
        "\n",
        "Dans ce projet, nous allons créer un réseau qui peut générer du texte, ici nous montrons que cela se fait caractère par caractère.\n"
      ],
      "metadata": {
        "id": "EKv5F-1M17l6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kKRTafNi1xzU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Étape 1 : Les Données  \n",
        "\n",
        "Vous pouvez récupérer n'importe quel texte gratuit ici : https://www.gutenberg.org/\n",
        "\n",
        "Nous allons choisir toutes les œuvres de Shakespeare, principalement pour deux raisons :\n",
        "\n",
        "1. C'est un grand corpus de textes, il est généralement recommandé d'avoir au moins une source d'un million de caractères au total pour obtenir une génération de texte réaliste.\n",
        "\n",
        "2. Il a un style très particulier. Comme les données textuelles utilisent un anglais ancien et sont formatées dans le style d'une pièce de théâtre, il nous apparaîtra très clairement si le modèle est capable de reproduire des résultats similaires."
      ],
      "metadata": {
        "id": "cCxHOvAZ2Xt7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iqW712mi1xza"
      },
      "outputs": [],
      "source": [
        "path_to_file = \"/content/sample_data/shakespeare.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OwIGAVKS1xzb"
      },
      "outputs": [],
      "source": [
        "text = open(path_to_file, 'r').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lISqxSSC1xzc",
        "outputId": "7e075828-3100-435b-90c3-e077560a5748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bu\n"
          ]
        }
      ],
      "source": [
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Comprendre les caractères uniques**"
      ],
      "metadata": {
        "id": "8Um4ghNw25KP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SOA8mnm-1xzd",
        "outputId": "cee42c57-d6a6-47fa-ccfb-99c862995d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Les caractères uniques dans le fichier\n",
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "len(vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etape 2: Traitement du Texte  \n",
        "* **Vectorisation du texte:**  \n",
        "  \n",
        "Nous savons qu'un réseau de neurones ne peut pas prendre en charge les données brutes des chaînes de caractères, nous deveons attribuer des numéros à chaque caractères. Créons deux dictionnaires qui peuvent passer d'un index numérique à un caractères et d'un caractère à un index numérique."
      ],
      "metadata": {
        "id": "lJFPDQRX3Sid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EemnfIVo1xzl",
        "outputId": "a6d1946e-3604-4706-9e17-649c4e1fdf94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " '(': 6,\n",
              " ')': 7,\n",
              " ',': 8,\n",
              " '-': 9,\n",
              " '.': 10,\n",
              " '0': 11,\n",
              " '1': 12,\n",
              " '2': 13,\n",
              " '3': 14,\n",
              " '4': 15,\n",
              " '5': 16,\n",
              " '6': 17,\n",
              " '7': 18,\n",
              " '8': 19,\n",
              " '9': 20,\n",
              " ':': 21,\n",
              " ';': 22,\n",
              " '<': 23,\n",
              " '>': 24,\n",
              " '?': 25,\n",
              " 'A': 26,\n",
              " 'B': 27,\n",
              " 'C': 28,\n",
              " 'D': 29,\n",
              " 'E': 30,\n",
              " 'F': 31,\n",
              " 'G': 32,\n",
              " 'H': 33,\n",
              " 'I': 34,\n",
              " 'J': 35,\n",
              " 'K': 36,\n",
              " 'L': 37,\n",
              " 'M': 38,\n",
              " 'N': 39,\n",
              " 'O': 40,\n",
              " 'P': 41,\n",
              " 'Q': 42,\n",
              " 'R': 43,\n",
              " 'S': 44,\n",
              " 'T': 45,\n",
              " 'U': 46,\n",
              " 'V': 47,\n",
              " 'W': 48,\n",
              " 'X': 49,\n",
              " 'Y': 50,\n",
              " 'Z': 51,\n",
              " '[': 52,\n",
              " ']': 53,\n",
              " '_': 54,\n",
              " '`': 55,\n",
              " 'a': 56,\n",
              " 'b': 57,\n",
              " 'c': 58,\n",
              " 'd': 59,\n",
              " 'e': 60,\n",
              " 'f': 61,\n",
              " 'g': 62,\n",
              " 'h': 63,\n",
              " 'i': 64,\n",
              " 'j': 65,\n",
              " 'k': 66,\n",
              " 'l': 67,\n",
              " 'm': 68,\n",
              " 'n': 69,\n",
              " 'o': 70,\n",
              " 'p': 71,\n",
              " 'q': 72,\n",
              " 'r': 73,\n",
              " 's': 74,\n",
              " 't': 75,\n",
              " 'u': 76,\n",
              " 'v': 77,\n",
              " 'w': 78,\n",
              " 'x': 79,\n",
              " 'y': 80,\n",
              " 'z': 81,\n",
              " '|': 82,\n",
              " '}': 83}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
        "char_to_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M9rFzN9Q1xzm",
        "outputId": "4310f85b-13e0-4afd-9339-bdc8367c213e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1',\n",
              "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?',\n",
              "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
              "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
              "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
              "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
              "       'w', 'x', 'y', 'z', '|', '}'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ind_to_char = np.array(vocab)\n",
        "ind_to_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ebuaqem11xzo",
        "outputId": "87382cc7-b048-4ded-9d24-36f53f3654ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  1, ..., 30, 39, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "encoded_text = np.array([char_to_ind[c] for c in text])\n",
        "encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HkFYBo511xzo",
        "outputId": "6ce9967a-c547-4cf0-ea6e-d3f1c22f34b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5445609,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "encoded_text.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Nous disposons maintenant d'un mapping que nous pouvons utiliser pour passer des caractères aux chiffres."
      ],
      "metadata": {
        "id": "sPoex78f5ZPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nrN8x2US1xzp",
        "outputId": "79b125fc-edef-46fd-ed81-c527d7e801f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n                   '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sample = text[:20]\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V9TVl9Ke1xzp",
        "outputId": "cbfa1405-57da-4696-b79f-55dcbdf3452a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "encoded_text[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Étape 3 : Création de batches  \n",
        "\n",
        "Globalement, ce que nous essayons de faire, c'est de faire en  sorte que le modèle prévoie le caractère suivant le plus probable, compte tenu d'une séquence historique de caractères. C'est à nous (l'utisateur) de choisir la longueur de cette séquence historique. Une séquence trop courte et nous n'aurons pas assez d'informations (par exemple, étant donné la lettre \"a\", quel est le ,prochaine caractère), une séquence trop longue et l'entrainement prendra trop de temps et risque de sur-entrainer (au riques d'obtenir une séquence de caractère qui ne sont pas pertinents pour des caractères plus éloignés). Bien qu'il n'y ait pas de choix correct de longueur de séquence, on dois considerer le texte lui-même, la longueur des phrases normales qu'il contient et avoir une idée raisonnable des caractères/mots qui ne sont pertinents les uns pour les autres."
      ],
      "metadata": {
        "id": "Z_f6LJsZ5w_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "id": "MCZxqmg97xfN",
        "outputId": "bf28d9cd-a582-4594-ec76-c04df7ef9c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ugok7xrN1xzp",
        "outputId": "0c6f6d66-fa17-4840-e546-91adf70a7df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "line = \"From fairest creatures we desire increase,\"\n",
        "len(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YbZtfwa_1xzq",
        "outputId": "ca5bfe25-b450-4d91-ff3d-d779865d3c9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "part_stanza = '''From fairest creatures we desire increase,\n",
        "  That thereby beauty's rose might never die,\n",
        "  But as the riper should by time decease,'''\n",
        "\n",
        "len(part_stanza)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sequences d'Entraînement\n",
        "Les données textuelles réelles seront la séquence de texte décalée d'un caractère vers l'avant.  \n",
        "\n",
        "Par exemple:  \n",
        "- Sequence In: \"Hello my name\"  \n",
        "- Sequence Out: \"ello my name\"  \n",
        "\n",
        "Nous pouvons utiliser la fonction `tf.data.Dataset.from_tensor_slices` pour convertir un vecteur de texte en un flux d'indices de caractères."
      ],
      "metadata": {
        "id": "ucJGGoLN8SX4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Lh0RXUn01xzq"
      },
      "outputs": [],
      "source": [
        "seq_len = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZSBbCijc1xzq",
        "outputId": "e14a4253-f5be-4e59-af0b-0893a4830172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45005"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "total_num_seq = len(text) // (seq_len + 1)\n",
        "total_num_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "djCKdG0l1xzr",
        "outputId": "5ac4717b-5875-426b-a438-e36f4f329d6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "1\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "F\n",
            "r\n",
            "o\n",
            "m\n",
            " \n",
            "f\n",
            "a\n",
            "i\n",
            "r\n",
            "e\n",
            "s\n",
            "t\n",
            " \n",
            "c\n",
            "r\n",
            "e\n",
            "a\n",
            "t\n",
            "u\n",
            "r\n",
            "e\n",
            "s\n",
            " \n",
            "w\n",
            "e\n",
            " \n",
            "d\n",
            "e\n",
            "s\n",
            "i\n",
            "r\n",
            "e\n",
            " \n",
            "i\n",
            "n\n",
            "c\n",
            "r\n",
            "e\n",
            "a\n",
            "s\n",
            "e\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "T\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            "b\n",
            "y\n",
            " \n",
            "b\n",
            "e\n",
            "a\n",
            "u\n",
            "t\n",
            "y\n",
            "'\n",
            "s\n",
            " \n",
            "r\n",
            "o\n",
            "s\n",
            "e\n",
            " \n",
            "m\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "n\n",
            "e\n",
            "v\n",
            "e\n",
            "r\n",
            " \n",
            "d\n",
            "i\n",
            "e\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "B\n",
            "u\n",
            "t\n",
            " \n",
            "a\n",
            "s\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "r\n",
            "i\n",
            "p\n",
            "e\n",
            "r\n",
            " \n",
            "s\n",
            "h\n",
            "o\n",
            "u\n",
            "l\n",
            "d\n",
            " \n",
            "b\n",
            "y\n",
            " \n",
            "t\n",
            "i\n",
            "m\n",
            "e\n",
            " \n",
            "d\n",
            "e\n",
            "c\n",
            "e\n",
            "a\n",
            "s\n",
            "e\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "H\n",
            "i\n",
            "s\n",
            " \n",
            "t\n",
            "e\n",
            "n\n",
            "d\n",
            "e\n",
            "r\n",
            " \n",
            "h\n",
            "e\n",
            "i\n",
            "r\n",
            " \n",
            "m\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "b\n",
            "e\n",
            "a\n",
            "r\n",
            " \n",
            "h\n",
            "i\n",
            "s\n",
            " \n",
            "m\n",
            "e\n",
            "m\n",
            "o\n",
            "r\n",
            "y\n",
            ":\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "B\n",
            "u\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "o\n",
            "u\n",
            " \n",
            "c\n",
            "o\n",
            "n\n",
            "t\n",
            "r\n",
            "a\n",
            "c\n",
            "t\n",
            "e\n",
            "d\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "t\n",
            "h\n",
            "i\n",
            "n\n",
            "e\n",
            " \n",
            "o\n",
            "w\n",
            "n\n",
            " \n",
            "b\n",
            "r\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "e\n",
            "y\n",
            "e\n",
            "s\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "F\n",
            "e\n",
            "e\n",
            "d\n",
            "'\n",
            "s\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "y\n",
            " \n",
            "l\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            "'\n",
            "s\n",
            " \n",
            "f\n",
            "l\n",
            "a\n",
            "m\n",
            "e\n",
            " \n",
            "w\n",
            "i\n",
            "t\n",
            "h\n",
            " \n",
            "s\n",
            "e\n",
            "l\n",
            "f\n",
            "-\n",
            "s\n",
            "u\n",
            "b\n",
            "s\n",
            "t\n",
            "a\n",
            "n\n",
            "t\n",
            "i\n",
            "a\n",
            "l\n",
            " \n",
            "f\n",
            "u\n",
            "e\n",
            "l\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "M\n",
            "a\n",
            "k\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "a\n",
            " \n",
            "f\n",
            "a\n",
            "m\n",
            "i\n",
            "n\n",
            "e\n",
            " \n",
            "w\n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "a\n",
            "b\n",
            "u\n",
            "n\n",
            "d\n",
            "a\n",
            "n\n",
            "c\n",
            "e\n",
            " \n",
            "l\n",
            "i\n",
            "e\n",
            "s\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "T\n",
            "h\n",
            "y\n",
            " \n",
            "s\n",
            "e\n",
            "l\n",
            "f\n",
            " \n",
            "t\n",
            "h\n",
            "y\n",
            " \n",
            "f\n",
            "o\n",
            "e\n",
            ",\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "t\n",
            "h\n",
            "y\n",
            " \n",
            "s\n",
            "w\n",
            "e\n",
            "e\n",
            "t\n",
            " \n",
            "s\n",
            "e\n",
            "l\n",
            "f\n",
            " \n",
            "t\n",
            "o\n",
            "o\n",
            " \n",
            "c\n",
            "r\n",
            "u\n",
            "e\n",
            "l\n",
            ":\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "T\n",
            "h\n",
            "o\n",
            "u\n",
            " \n",
            "t\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "a\n",
            "r\n",
            "t\n",
            " \n",
            "n\n",
            "o\n",
            "w\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "w\n",
            "o\n",
            "r\n",
            "l\n",
            "d\n",
            "'\n",
            "s\n",
            " \n",
            "f\n",
            "r\n",
            "e\n",
            "s\n",
            "h\n",
            " \n",
            "o\n",
            "r\n",
            "n\n",
            "a\n",
            "m\n",
            "e\n",
            "n\n",
            "t\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "A\n",
            "n\n",
            "d\n",
            " \n",
            "o\n",
            "n\n",
            "l\n",
            "y\n",
            " \n",
            "h\n",
            "e\n",
            "r\n",
            "a\n",
            "l\n",
            "d\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "g\n",
            "a\n",
            "u\n",
            "d\n",
            "y\n",
            " \n",
            "s\n",
            "p\n",
            "r\n",
            "i\n",
            "n\n",
            "g\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "W\n",
            "i\n",
            "t\n",
            "h\n",
            "i\n",
            "n\n",
            " \n",
            "t\n",
            "h\n",
            "i\n",
            "n\n",
            "e\n",
            " \n",
            "o\n",
            "w\n",
            "n\n",
            " \n",
            "b\n",
            "u\n"
          ]
        }
      ],
      "source": [
        "# Création des séquences d'entrainement\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "for item in char_dataset.take(500):\n",
        "    print(ind_to_char[item.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La méthode du **batch** convertit ces appels de caractères individuels en séquences que nous pouvons alimenter en lot. Nous utilions seq_len+1 en raison de l'indexation zéro.  \n",
        "\n",
        "Voici ce que signifie drop_remainder:  \n",
        "\n",
        "*drop_remainder*: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   "
      ],
      "metadata": {
        "id": "0UyvctLS94bW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t4VTQZPg1xzr",
        "outputId": "32b3388c-cf5d-4611-bd7e-f7554d29ac02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "type(char_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "JgChxBvo_LYq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que nous avons nos séquences, nous allons effectuer les étapes suivantes pour chacune d'entre elles afin de créer nos séquences de texte cible.  \n",
        "1. Saisir la séquence de texte d'entrée  \n",
        "2. Assigner la séquence de texte cible comme séquence de texte d'entrée décalée d'un pas en avant.  \n",
        "3. Regroupez-les en un tuple  \n"
      ],
      "metadata": {
        "id": "yyfw1BwQ_bDs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YX3R_FDD1xzs"
      },
      "outputs": [],
      "source": [
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1]\n",
        "    target_txt = seq[1:]\n",
        "    return input_txt, target_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pmi7V-4o1xzs"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(create_seq_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mPksC6Pf1xzs",
        "outputId": "d74446a8-c5cc-4b3e-b8ca-7f5813141f0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
            "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But\n",
            "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
            "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
            " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
            " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
            "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But \n"
          ]
        }
      ],
      "source": [
        "for input_txt, target_txt in dataset.take(1):\n",
        "    print(input_txt.numpy())\n",
        "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
        "    print(target_txt.numpy())\n",
        "    print(''.join(ind_to_char[target_txt.numpy()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Générer des batches d'entrainement  \n",
        "\n",
        "Maintenant que nous avons les séquences réelles, nous allons créer les lots, nous voulons mélanger ces séquences dans un ordre aléatoire, de sorte que le modèle ne s'adapte à aucune section du texte, mais puisse au contraire générer des caractère à partir de n'importe quel texte de départ.  \n"
      ],
      "metadata": {
        "id": "CGQlwvglAOl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vKnKz4421xzs"
      },
      "outputs": [],
      "source": [
        "# Taille de batch\n",
        "batch_size = 128\n",
        "\n",
        "# Taille de la mémoire tampon pour mélanger l'ensemble des données\n",
        "# afin de ne pas tenter de mélanger toute la séquence en mémoire.\n",
        "# Au lieu de cela, il maintient une mémoire tampon dans laquelle\n",
        "# il mélange les éléments\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "E58jzsC_1xzs",
        "outputId": "068159f1-e092-49f3-f021-2be47e407d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(128, 120), dtype=tf.int64, name=None), TensorSpec(shape=(128, 120), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etape 4: Création du modèle  \n",
        "\n",
        "Nous utiliserons un modèle basé sur le LSTM avec quelques caractéristiques supplémentaires, notamment une couche embedding pour commencer et deux couches LSTM. Nous avons basé cette architecture de modèle sur [DeepMoji](https://deepmoji.mit.edu/) et le code source original peut être trouvé [ici](https://github.com/bfelbo/DeepMoji).\n",
        "\n",
        "La couche embedding servira de couche d'entrée, qui crée essentiellement une table de consultation qui fait correspondre les indices numériques de chaque caractères à un vecteur avec un nombre de dimensions \"embedding dim\". Comme on peut l'imaginer, plus cette taille d'embedding est grande, plus l'entraînement est complexe. C'est similaire à l'idée derrière word2vec, oû les mots sont mis en correspondance avec un espace à n dimensions. L'embedding avant le feeding direct dans le LSTM conduit généralement à des résultats plus réalistes."
      ],
      "metadata": {
        "id": "Z76VskWeBZWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WrDfF1IH1xzt"
      },
      "outputs": [],
      "source": [
        "# Longueur du vocabulaire en caractères\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# La dimension embedding\n",
        "embed_dim = 64\n",
        "\n",
        "# Nombre d'unitées RNN\n",
        "rnn_neurones = 1026"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons maintenant une fonction qui s'adapte facilement aux différentes variables comme indiqué ci-dessus."
      ],
      "metadata": {
        "id": "oMwDIdFYDc7p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3Fa24gAN1xzu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, GRU, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mise en place de la fonction de perte  \n",
        "\n",
        "Pour notre part, nous utiliserons une crossentropie catégorique peu dense (sparse_categorical_crossentropy), que nous pouvons importer de Keras. Nous définirons également ce paramètre `logits=True`"
      ],
      "metadata": {
        "id": "U4PXZn7mD8hx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6kVCYZZc1xzu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(sparse_categorical_crossentropy)"
      ],
      "metadata": {
        "id": "O91mKC81EcyS",
        "outputId": "7a5be31e-eff8-487c-e1eb-3eac2ddb70d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function sparse_categorical_crossentropy in module keras.src.losses:\n",
            "\n",
            "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1, ignore_class=None)\n",
            "    Computes the sparse categorical crossentropy loss.\n",
            "    \n",
            "    Standalone usage:\n",
            "    \n",
            "    >>> y_true = [1, 2]\n",
            "    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
            "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
            "    >>> assert loss.shape == (2,)\n",
            "    >>> loss.numpy()\n",
            "    array([0.0513, 2.303], dtype=float32)\n",
            "    \n",
            "    >>> y_true = [[[ 0,  2],\n",
            "    ...            [-1, -1]],\n",
            "    ...           [[ 0,  2],\n",
            "    ...            [-1, -1]]]\n",
            "    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n",
            "    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n",
            "    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n",
            "    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n",
            "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
            "    ...   y_true, y_pred, ignore_class=-1)\n",
            "    >>> loss.numpy()\n",
            "    array([[[2.3841855e-07, 2.3841855e-07],\n",
            "            [0.0000000e+00, 0.0000000e+00]],\n",
            "           [[2.3841855e-07, 6.9314730e-01],\n",
            "            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n",
            "    \n",
            "    Args:\n",
            "        y_true: Ground truth values.\n",
            "        y_pred: The predicted values.\n",
            "        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
            "            default, we assume that `y_pred` encodes a probability distribution.\n",
            "        axis: Defaults to -1. The dimension along which the entropy is\n",
            "            computed.\n",
            "        ignore_class: Optional integer. The ID of a class to be ignored during\n",
            "            loss computation. This is useful, for example, in segmentation\n",
            "            problems featuring a \"void\" class (commonly -1 or 255) in\n",
            "            segmentation maps. By default (`ignore_class=None`), all classes are\n",
            "            considered.\n",
            "    \n",
            "    Returns:\n",
            "        Sparse categorical crossentropy loss value.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy"
      ],
      "metadata": {
        "id": "CRqjqnBqEoX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LbrMcnbD1xzu"
      },
      "outputs": [],
      "source": [
        "def sparse_cat_loss(y_true, y_pred):\n",
        "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1qy4Odvo1xzu"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, embed_dim, rnn_neurones, batch_size):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\n",
        "\n",
        "    model.add(GRU(rnn_neurones, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "    # Couche Finale Dense de Prédiction\n",
        "    model.add(Dense(vocab_size))\n",
        "\n",
        "    model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EXq2uwKR1xz1",
        "outputId": "2b883bf0-aa6e-4f02-943d-3c696a0781d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (128, None, 64)           5376      \n",
            "                                                                 \n",
            " gru (GRU)                   (128, None, 1026)         3361176   \n",
            "                                                                 \n",
            " dense (Dense)               (128, None, 84)           86268     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3452820 (13.17 MB)\n",
            "Trainable params: 3452820 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    rnn_neurones=rnn_neurones,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etape 5: Entrainement du modèle  \n",
        "\n",
        "Assurons-nous que tout va bien avec notre modèle avant de passer trop de temps sur l'entrainement! Passons en lot pour confirmer que le modèle prédit actuellement des caractères aléatoires sans aucun entraînement.  "
      ],
      "metadata": {
        "id": "wtpYdTlGF6Qm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZXIcySWF1xz1",
        "outputId": "b96148d3-2daf-46a4-8160-e1b067a8f280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 120, 84) <=== (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    # Prédire sur un lot aléatoire\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "    # Afficher les dimensions des prédictions\n",
        "    print(example_batch_predictions.shape, \"<=== (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions"
      ],
      "metadata": {
        "id": "EpUMydvBHLUF",
        "outputId": "3a609152-b61d-4390-eb56-2d5b03db51ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 120, 84), dtype=float32, numpy=\n",
              "array([[[ 6.45444321e-04,  5.04791038e-04,  9.46331071e-04, ...,\n",
              "         -2.29320372e-03, -1.50430982e-03, -1.02553349e-02],\n",
              "        [ 1.56125217e-03,  1.53367326e-03,  5.93342469e-04, ...,\n",
              "         -3.12202028e-03, -3.58828134e-03, -1.59327406e-02],\n",
              "        [ 2.18758220e-03,  2.42294162e-03, -7.60422263e-05, ...,\n",
              "         -3.29200062e-03, -5.17784804e-03, -1.90665368e-02],\n",
              "        ...,\n",
              "        [ 5.17163286e-03,  3.48935835e-03,  2.21086317e-03, ...,\n",
              "         -6.99123787e-03, -9.41390730e-03, -6.25078240e-03],\n",
              "        [ 3.09444987e-03, -3.69275780e-03, -5.27907454e-04, ...,\n",
              "         -8.26895994e-04, -6.68878993e-03, -4.17404063e-03],\n",
              "        [-9.83467442e-04, -1.68880937e-03,  6.37631537e-03, ...,\n",
              "         -6.40799699e-04, -4.49449569e-03, -4.07541869e-03]],\n",
              "\n",
              "       [[ 6.45444321e-04,  5.04791038e-04,  9.46331071e-04, ...,\n",
              "         -2.29320372e-03, -1.50430982e-03, -1.02553349e-02],\n",
              "        [ 1.56125217e-03,  1.53367326e-03,  5.93342469e-04, ...,\n",
              "         -3.12202028e-03, -3.58828134e-03, -1.59327406e-02],\n",
              "        [-2.99640279e-03,  5.33616228e-04, -4.50790441e-03, ...,\n",
              "         -5.43049863e-03,  6.32699486e-03, -5.23158815e-03],\n",
              "        ...,\n",
              "        [ 5.21374866e-03,  3.66407284e-03,  4.62203607e-04, ...,\n",
              "          2.63899588e-03, -2.15751422e-03, -6.67020027e-03],\n",
              "        [ 1.06521985e-02, -1.34445226e-03, -7.09159358e-04, ...,\n",
              "         -7.04185758e-03, -5.22993505e-03, -1.09130749e-02],\n",
              "        [ 1.28625939e-02, -3.83669767e-03, -1.64434989e-03, ...,\n",
              "         -1.07497955e-02, -7.51746306e-03, -1.34437606e-02]],\n",
              "\n",
              "       [[ 6.22808700e-03, -4.12821304e-03,  1.10638408e-04, ...,\n",
              "         -8.36553331e-03, -4.34168894e-03, -8.02499801e-03],\n",
              "        [ 1.91997027e-03,  1.43288157e-03,  7.66180409e-03, ...,\n",
              "         -5.53349359e-03, -6.16684509e-03, -3.53169232e-03],\n",
              "        [ 1.24913349e-03, -9.93732130e-04,  1.15462195e-03, ...,\n",
              "         -5.45937195e-03, -5.19575551e-03, -2.80166441e-03],\n",
              "        ...,\n",
              "        [ 1.96562172e-03, -8.30914918e-03,  3.83172766e-03, ...,\n",
              "         -1.25955290e-03, -3.77047528e-03, -9.39598831e-04],\n",
              "        [ 1.91237510e-03, -3.13495123e-03,  2.68119969e-03, ...,\n",
              "         -3.35398503e-03, -3.07397987e-03, -1.01255327e-02],\n",
              "        [ 4.36366582e-03, -2.80605746e-04,  5.23497118e-03, ...,\n",
              "         -7.71258911e-03, -6.49006711e-03, -9.02151922e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 5.60887204e-03, -1.33007753e-03,  2.12892983e-03, ...,\n",
              "         -2.08040839e-03,  3.00468411e-04,  5.58277778e-03],\n",
              "        [ 1.01631051e-02, -5.24025457e-03,  1.70106092e-03, ...,\n",
              "         -7.84901995e-03, -4.82145511e-03, -4.97672148e-03],\n",
              "        [ 7.82436598e-03, -3.63040563e-05, -1.87391031e-03, ...,\n",
              "          1.07733638e-03,  2.46156729e-03, -5.99927502e-03],\n",
              "        ...,\n",
              "        [ 5.65453852e-03, -4.69736662e-03,  4.90160659e-04, ...,\n",
              "          2.01255549e-03, -5.03254635e-03, -3.34642711e-03],\n",
              "        [ 9.54990461e-03, -6.77362876e-03,  1.67450489e-04, ...,\n",
              "         -6.04445348e-03, -7.17837177e-03, -1.01948641e-02],\n",
              "        [ 6.02127658e-03, -2.53627775e-03,  2.84725771e-04, ...,\n",
              "         -3.81320692e-03, -5.88878011e-03, -1.58833135e-02]],\n",
              "\n",
              "       [[-2.73926626e-03, -1.05680327e-03,  7.63082039e-03, ...,\n",
              "         -1.94211359e-04, -1.18483254e-03, -2.51543475e-03],\n",
              "        [-6.20770734e-04,  4.47781029e-04,  5.12280967e-03, ...,\n",
              "         -3.13082454e-03, -1.75805227e-03, -1.18284253e-02],\n",
              "        [-1.50215847e-03, -9.76743875e-04,  8.05083290e-03, ...,\n",
              "         -4.38430160e-03, -4.29939060e-03, -6.13513030e-03],\n",
              "        ...,\n",
              "        [-1.48111535e-03,  2.72077927e-03,  7.29166716e-03, ...,\n",
              "         -5.41290734e-03, -2.47732736e-03, -1.46902101e-02],\n",
              "        [-1.83214515e-03,  5.32117905e-03,  1.13255838e-02, ...,\n",
              "         -5.47807198e-03, -5.51719591e-03, -8.03741161e-03],\n",
              "        [-6.27926923e-03,  2.96373945e-03,  5.89095475e-03, ...,\n",
              "          2.41587381e-03, -2.94351531e-03, -4.81247483e-03]],\n",
              "\n",
              "       [[ 5.66610694e-03, -2.00357614e-03, -3.86634446e-03, ...,\n",
              "         -3.05097387e-03, -4.27298015e-03, -3.49530019e-03],\n",
              "        [-1.77566824e-03, -1.91663718e-03, -5.67589421e-03, ...,\n",
              "         -4.51426767e-03,  8.47230852e-03,  2.24269321e-03],\n",
              "        [ 4.06108418e-04,  1.28938351e-03, -2.05066032e-03, ...,\n",
              "         -4.64518880e-03,  2.93519674e-03, -7.92866480e-03],\n",
              "        ...,\n",
              "        [-1.98775670e-03,  5.64996712e-03, -1.75444479e-03, ...,\n",
              "         -7.20107183e-03,  8.60215724e-03, -1.33455861e-02],\n",
              "        [ 6.04808971e-04,  1.67895178e-03, -6.65936805e-03, ...,\n",
              "         -7.07817636e-03,  7.67684821e-03, -1.00341635e-02],\n",
              "        [ 1.93180982e-03,  4.66076797e-03, -2.89811264e-03, ...,\n",
              "         -1.87074614e-03,  6.70372276e-04, -7.60197919e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nnYNDVhH1xz2"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "f3TME6101xz2",
        "outputId": "68a229a2-8b5d-4f25-b2ec-7dd00ee24fa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(120, 1), dtype=int64, numpy=\n",
              "array([[44],\n",
              "       [40],\n",
              "       [25],\n",
              "       [80],\n",
              "       [50],\n",
              "       [30],\n",
              "       [26],\n",
              "       [ 9],\n",
              "       [65],\n",
              "       [30],\n",
              "       [23],\n",
              "       [27],\n",
              "       [67],\n",
              "       [40],\n",
              "       [76],\n",
              "       [48],\n",
              "       [39],\n",
              "       [ 9],\n",
              "       [17],\n",
              "       [ 8],\n",
              "       [38],\n",
              "       [21],\n",
              "       [36],\n",
              "       [48],\n",
              "       [52],\n",
              "       [43],\n",
              "       [68],\n",
              "       [59],\n",
              "       [48],\n",
              "       [ 5],\n",
              "       [ 1],\n",
              "       [81],\n",
              "       [68],\n",
              "       [76],\n",
              "       [47],\n",
              "       [40],\n",
              "       [76],\n",
              "       [60],\n",
              "       [47],\n",
              "       [ 0],\n",
              "       [ 5],\n",
              "       [81],\n",
              "       [ 5],\n",
              "       [17],\n",
              "       [55],\n",
              "       [29],\n",
              "       [42],\n",
              "       [ 5],\n",
              "       [63],\n",
              "       [64],\n",
              "       [54],\n",
              "       [20],\n",
              "       [17],\n",
              "       [30],\n",
              "       [62],\n",
              "       [ 8],\n",
              "       [24],\n",
              "       [31],\n",
              "       [62],\n",
              "       [74],\n",
              "       [ 9],\n",
              "       [47],\n",
              "       [13],\n",
              "       [44],\n",
              "       [63],\n",
              "       [42],\n",
              "       [50],\n",
              "       [58],\n",
              "       [60],\n",
              "       [17],\n",
              "       [75],\n",
              "       [40],\n",
              "       [21],\n",
              "       [78],\n",
              "       [77],\n",
              "       [18],\n",
              "       [78],\n",
              "       [14],\n",
              "       [38],\n",
              "       [46],\n",
              "       [61],\n",
              "       [44],\n",
              "       [34],\n",
              "       [35],\n",
              "       [30],\n",
              "       [ 1],\n",
              "       [82],\n",
              "       [ 6],\n",
              "       [29],\n",
              "       [22],\n",
              "       [80],\n",
              "       [39],\n",
              "       [19],\n",
              "       [59],\n",
              "       [53],\n",
              "       [39],\n",
              "       [ 8],\n",
              "       [35],\n",
              "       [30],\n",
              "       [52],\n",
              "       [48],\n",
              "       [81],\n",
              "       [81],\n",
              "       [58],\n",
              "       [ 8],\n",
              "       [56],\n",
              "       [36],\n",
              "       [83],\n",
              "       [41],\n",
              "       [63],\n",
              "       [28],\n",
              "       [ 2],\n",
              "       [26],\n",
              "       [48],\n",
              "       [60],\n",
              "       [27],\n",
              "       [71],\n",
              "       [46],\n",
              "       [76],\n",
              "       [62]])>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "1cIArDVm1xz2"
      },
      "outputs": [],
      "source": [
        "# Reformater pour ne pas être une liste de listes\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SV2zU-qd1xz2",
        "outputId": "b5739d88-8f5a-44f9-cc4f-47ad0fbf12c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44, 40, 25, 80, 50, 30, 26,  9, 65, 30, 23, 27, 67, 40, 76, 48, 39,\n",
              "        9, 17,  8, 38, 21, 36, 48, 52, 43, 68, 59, 48,  5,  1, 81, 68, 76,\n",
              "       47, 40, 76, 60, 47,  0,  5, 81,  5, 17, 55, 29, 42,  5, 63, 64, 54,\n",
              "       20, 17, 30, 62,  8, 24, 31, 62, 74,  9, 47, 13, 44, 63, 42, 50, 58,\n",
              "       60, 17, 75, 40, 21, 78, 77, 18, 78, 14, 38, 46, 61, 44, 34, 35, 30,\n",
              "        1, 82,  6, 29, 22, 80, 39, 19, 59, 53, 39,  8, 35, 30, 52, 48, 81,\n",
              "       81, 58,  8, 56, 36, 83, 41, 63, 28,  2, 26, 48, 60, 27, 71, 46, 76,\n",
              "       62])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "eRV1HUDa1xz3",
        "outputId": "15757c05-7cca-462f-f1a7-3b3e92b2b6a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compte tenu de la séquence d'entrée : \n",
            "\n",
            "    [Puts down the skull.]\n",
            "  Hor. E'en so, my lord.\n",
            "  Ham. To what base uses we may return, Horatio! Why may not\n",
            "    ima\n",
            "\n",
            "\n",
            "Prochain caractère prédit : \n",
            "\n",
            "SO?yYEA-jE<BlOuWN-6,M:KW[RmdW' zmuVOueV\n",
            "'z'6`DQ'hi_96Eg,>Fgs-V2ShQYce6tO:wv7w3MUfSIJE |(D;yN8d]N,JE[Wzzc,aK}PhC!AWeBpUug\n"
          ]
        }
      ],
      "source": [
        "print(\"Compte tenu de la séquence d'entrée : \\n\")\n",
        "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
        "print(\"\\n\")\n",
        "print(\"Prochain caractère prédit : \\n\")\n",
        "print(\"\".join(ind_to_char[sampled_indices]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après avoir confirmé que les dimensions fonctionnent, entraînons notre réseau !"
      ],
      "metadata": {
        "id": "jaYTz7EYJCvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-ce2znC01xz3"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wS9etfvP1xz3",
        "outputId": "6f12f68e-566e-4dd7-925d-dfb38338ef29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "351/351 [==============================] - 49s 125ms/step - loss: 2.5182\n",
            "Epoch 2/30\n",
            "351/351 [==============================] - 46s 122ms/step - loss: 1.7117\n",
            "Epoch 3/30\n",
            "351/351 [==============================] - 47s 129ms/step - loss: 1.4497\n",
            "Epoch 4/30\n",
            "351/351 [==============================] - 48s 132ms/step - loss: 1.3347\n",
            "Epoch 5/30\n",
            "351/351 [==============================] - 48s 131ms/step - loss: 1.2739\n",
            "Epoch 6/30\n",
            "351/351 [==============================] - 50s 136ms/step - loss: 1.2344\n",
            "Epoch 7/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.2060\n",
            "Epoch 8/30\n",
            "351/351 [==============================] - 49s 134ms/step - loss: 1.1825\n",
            "Epoch 9/30\n",
            "351/351 [==============================] - 50s 138ms/step - loss: 1.1627\n",
            "Epoch 10/30\n",
            "351/351 [==============================] - 49s 133ms/step - loss: 1.1452\n",
            "Epoch 11/30\n",
            "351/351 [==============================] - 48s 131ms/step - loss: 1.1295\n",
            "Epoch 12/30\n",
            "351/351 [==============================] - 51s 136ms/step - loss: 1.1146\n",
            "Epoch 13/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.1011\n",
            "Epoch 14/30\n",
            "351/351 [==============================] - 49s 134ms/step - loss: 1.0888\n",
            "Epoch 15/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0768\n",
            "Epoch 16/30\n",
            "351/351 [==============================] - 49s 134ms/step - loss: 1.0656\n",
            "Epoch 17/30\n",
            "351/351 [==============================] - 51s 137ms/step - loss: 1.0544\n",
            "Epoch 18/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0442\n",
            "Epoch 19/30\n",
            "351/351 [==============================] - 49s 135ms/step - loss: 1.0350\n",
            "Epoch 20/30\n",
            "351/351 [==============================] - 52s 138ms/step - loss: 1.0261\n",
            "Epoch 21/30\n",
            "351/351 [==============================] - 50s 138ms/step - loss: 1.0184\n",
            "Epoch 22/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0109\n",
            "Epoch 23/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0036\n",
            "Epoch 24/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 0.9974\n",
            "Epoch 25/30\n",
            "351/351 [==============================] - 52s 139ms/step - loss: 0.9917\n",
            "Epoch 26/30\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 0.9869\n",
            "Epoch 27/30\n",
            "351/351 [==============================] - 51s 140ms/step - loss: 0.9831\n",
            "Epoch 28/30\n",
            "351/351 [==============================] - 51s 141ms/step - loss: 0.9785\n",
            "Epoch 29/30\n",
            "351/351 [==============================] - 51s 140ms/step - loss: 0.9748\n",
            "Epoch 30/30\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 0.9722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b5a8226efb0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "model.fit(dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etape 6: Génération du texte\n",
        "\n",
        "Actuellement, notre modéle ne prévoit que 128 séquences à la fois. Nous pouvons créer un nouveau modèle qui n'attend qu'un batch_size=1.  \n",
        "Nous pouvons créer un nouveau modèle avec cette taille de batch, puis charger les poids de nos modèles sauvegardés. Ensuite, appelez `.build()` sur le model:"
      ],
      "metadata": {
        "id": "Q54RYdMGTYgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4rL6BxJf1xz3",
        "outputId": "a9a3fbb5-29b4-4ab0-d0b9-079daf1f8c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"shakespeare_gen.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "WvYnNcV1UIog"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(vocab_size, embed_dim, rnn_neurones, batch_size=1)\n",
        "\n",
        "model.load_weights('shakespeare_gen.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "QvSf6LfcUVjA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "FYwCtjzVUrid",
        "outputId": "bf8e81cb-407c-4f92-d9bf-980fe83b681d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 64)             5376      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1026)           3361176   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 84)             86268     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3452820 (13.17 MB)\n",
            "Trainable params: 3452820 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_seed, gen_size=100, temp=1.0):\n",
        "  '''\n",
        "  model: Modèle Entraîné pour générer du texte\n",
        "  start_seed: Seed initial du texte sous forme de chaîne de caractères\n",
        "  gen_size: Nombre de caractères à générer\n",
        "\n",
        "  L'idée de base de cette fonction est de prendre un texte de départ, de le formater de manière à\n",
        "  q'il soit dans le bon format pour notre réseau, puis bouclez la séquence à mesure que nous\n",
        "  continuons d'ajouter nos propres caractères prédits. Similaire à notre travail au sein des problèmes\n",
        "  de séries temporelles avec le RNN.\n",
        "  '''\n",
        "\n",
        "  # Nombre de caractères à générer\n",
        "  num_generate = gen_size\n",
        "\n",
        "  # Vectorisation du texte du seed de départ\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "\n",
        "  # Etendre les dimensions pour correspondre à la forme du format de batch\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Liste vide pour contenir le texte généré\n",
        "  text_generated = []\n",
        "\n",
        "  # La température a un effet aléatoire sur le texte qui en résulte\n",
        "  # Le terme est dérivé de l'entropie/thermodynamique.\n",
        "  # La temperature est utilisée pour affecter la probabilité des caractères suivants.\n",
        "  # Probabilité plus élevée == moins surprenante/ plus attendue\n",
        "  # Une température plus basse == plus surprenante / moins attendue\n",
        "\n",
        "  temperature = temp\n",
        "\n",
        "  # Ici batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "    # Générer des prédictions\n",
        "    predictions = model(input_eval)\n",
        "\n",
        "    # Supprimer la dimension de la forme du batch\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # Utilisez une distribution catégorielle pour sélectionner le caractère suivant\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # Passez le caractère prédit pour la prochaine entrée\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    # Transformer à nouveau en lettre de caractère\n",
        "    text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + ''.join(text_generated))\n"
      ],
      "metadata": {
        "id": "Toz0vtELUuzv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, \"flower\", gen_size=1000))"
      ],
      "metadata": {
        "id": "sPlwVp-O5YtV",
        "outputId": "7e8c7048-9d32-4c6b-bd1e-162061852c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flowers' sense\n",
            "    And the unparrotake and bring your Highness- look between you, it\n",
            "    were good to be knight; I had rather cry she hers.\n",
            "  POLIXENES. He replies me too. Saying oppose in silence, Prince of my life, and sleeps on me defens the aful case hath not won thy worst with\n",
            "    a vising frown.\n",
            "  POLIXENES. That's as such as you must be friends;\n",
            "    and as f that the world d doe to get forth,\n",
            "    For then it hath true privilege of worth from her\n",
            "    justifulate that have shall be in her life.\n",
            "  AUFIDIUS. I am all the days less it I go away.\n",
            "  MOTH. Sir Welch, sir, your legs are liming.\n",
            "  ANTONIO. Do you see away the longer hove?\n",
            "  DUKE. Dead, draw.\n",
            "                    [GAINTE and PHOLUS\n",
            "O Lo, what a speed? Is  \n",
            "    had never may not grinvellou with actions.\n",
            "  ANTONIO. Dost thou think, Desdemona is           Exit\n",
            "  OTHELLO. Thou liest; thy name and Merculio. When we lov'd more.\n",
            "\n",
            "  'MIDEA. That's not so.\n",
            "\n",
            "                        Re-enter Troilus,\n",
            "                    In open matters make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmwwJ7PV5iU2"
      },
      "execution_count": 49,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}